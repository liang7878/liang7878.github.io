---
title: 大语言模型的前世今生
date: 2023-11-28 21:28:30
tags:
  - Paper Reading
  - 数据库
categories: 论文阅读
---

2022 年底，ChatGPT 横空出世，一下子拉近了普通人和 AI 的距离。我对 AI 的印象还停留在几年前，AlphaGo大战李世石，转眼间，LLM 好像马上就要改变（或者说已经改变）大家的生活。作为技术从业者、这一批或者下一批要被AI 抢走工作的人，听着周围的同事积极讨论 LLM 相关的技术进展和可能的应用，我找了一些文章，想看看大语言模型（Large Language Model）是个什么。

## 一切从seq2seq谈起
![[Pasted image 20231209115716.png]]
Sequence-to-sequence(seq2seq)是一种深度学习模型架构，最开始由Ilya Sutskever

## RNN和LSTM
循环神经网络（Recurrent Neural Networks，RNN）和长短期记忆网络（Long Short-Term Memory，LSTM）都是深度学习中处理序列数据的重要工具。  
1. **循环神经网络（RNN）**：RNN 是一种专门处理序列数据的神经网络。不同于常规神经网络，RNN 的神经元之间是有循环连接的，这使得它可以处理任意长度的序列数据。RNN 的一个主要优点是其模型结构的灵活性，可以应用于各种任务，如语音识别、文本生成等。然而，RNN 也存在一些问题，如梯度消失/爆炸问题，难以处理长序列等。  
2. **长短期记忆网络（LSTM）**：LSTM 是 RNN 的一种变体，它通过引入"门"的概念来解决 RNN 的长期依赖问题。LSTM 有三个门：输入门、遗忘门和输出门，这三个门分别控制信息的输入、保存和输出。由于这种设计，LSTM 能够在处理长序列和保留长期依赖关系时表现得更好。然而，LSTM 的复杂性比 RNN 高，需要更多的计算资源和训练时间。

RNN存在以下问题：
1. **梯度消失问题**：这是RNN中的一个常见问题。在反向传播（网络从错误中学习）过程中，权重的变化与梯度的值成正比。对于长序列，这些梯度可能变得极小（实际上消失），这会减慢学习速度，或者使学习完全停止。  
2. **长序列**：RNN在处理长序列数据时有困难。它们难以将序列早期点的信息带到后面的点，这意味着它们在涉及长期依赖的任务上表现通常较差。  
3. **训练时间**：训练RNN可能需要很长时间，特别是对于大型网络和数据集。这是由于它们的循环性质，不允许在不同的时间步骤中并行计算。  
4. **过拟合**：RNN容易过拟合，尤其是在小型数据集上训练时。过拟合发生在模型过于了解训练数据，以至于在未见过的数据上表现不佳。
为了解决其中一些问题，特别是梯度消失问题和长期依赖问题，LSTM在RNN的基础上做了许多改进，但同时依然存在着一些问题：  
1. **复杂性**：LSTM比标准的RNN更复杂，这使得它们更难训练。它们有更多的参数，意味着它们需要更多的计算资源和训练时间。  
2. **过拟合**：像RNN一样，LSTM也可能过拟合，特别是当数据集很小的时候。  
3. **长训练时间**：由于它们的复杂性和参数多，LSTM可能需要很长的训练时间，特别是对于大型网络和数据集。  
## Attention Is All You Need
为了解决循环神经网络和LSTM网络在处理序列数据时效率过低等问题，Google研究团队在2017年发表了一篇《Attention Is All You Need》的文章，文章中提出了Transformer模型，这个模型完全摒弃了循环，转而通过注意力机制来在输入和输出之间建立全局依赖关系。这一改进使整体的训练效率获得显著的提升。

### Transformer模型架构
### 编码器和解码器
![[Pasted image 20231210104338.png]]
上图是Transformer模型的整体架构，架构基于编码器-解码器结构实现，即，编码器将输入的符号序列映射到一个连续的向量表示，解码器基于此生成一个输出序列。在上图的架构中，左边和右边分别是编码器和解码器：
- **编码器**：编码器由6个相同的层堆叠而成。每一层都有两个子层。第一个是多头自注意力机制，第二个是简单的位置全连接前馈网络。每个子层的输出是LayerNorm(x + Sublayer(x))，其中Sublayer(x)是子层本身实现的函数。
- **解码器**：解码器也由6个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，它对编码器堆栈的输出执行多头注意力。

### 注意力机制
一个典型的注意力函数会接收一个查询（query）和一组键值对（key-value pairs），然后计算查询与每个键之间的相似度。这些相似度分数通过softmax函数转换成权重，这些权重反映了每个键值对对于查询的重要性。

**缩放点积注意力函数：**
$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})$

缩放点积注意力函数是对点积注意力函数的改进，引入缩放因子$\frac{1}{\sqrt{d_k}}$是为了平衡输入序列长度对注意力分数计算的影响，以防止由于序列长度较大导致的梯度消失或爆炸问题。

**多头注意力**：$MultiHead(Q,K,V)=Concat(head_1, ..., head_h)W^O \,where\, head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$
其计算过程如下：

1. **计算分数（Score Calculation）：** 对于每个注意力头，使用缩放点积注意力函数计算查询和键之间的分数，即 $\text{Score}_{\text{head}}(Q, K) = \frac{Q \cdot K}{\sqrt{d_k}}$。

2. **计算权重（Weight Calculation）：** 对每个头计算注意力权重，将分数通过 softmax 函数进行归一化。

3. **加权求和（Weighted Sum）：** 使用注意力权重对值进行加权求和，即 $\text{Attention}_{\text{head}}(Q, K, V) = \text{Softmax}(\text{Score}_{\text{head}}(Q, K)) \cdot V$。

4. **多头拼接（Multi-Head Concatenation）：** 将多个注意力头的输出按照通道维度进行拼接。
5. **线性映射（Linear Mapping）：** 将拼接后的多头输出通过线性映射投影到最终的表示空间。

在 Transformer 模型中，多头注意力机制应用于不同的层和模块，以提高模型的表达能力。以下是 Transformer 中多头注意力的主要应用：

1. **编码器-解码器注意力（Encoder-Decoder Attention）：**
   - 在 Transformer 的解码器中，每个解码器层包含一个“编码器-解码器注意力”层。在这里，查询来自前一个解码器层，而内存的键和值来自编码器的输出。这样，解码器中的每个位置都可以关注输入序列的所有位置。这种注意力机制允许模型在生成每个解码器位置的时候，参考整个输入序列。

2. **编码器的自注意力（Encoder Self-Attention）：**
   - 在编码器中，每个编码器层都包含自注意力层。在自注意力中，查询、键和值都来自同一位置，即前一层的编码器输出。这使得每个编码器位置都能够关注其前一层的所有位置，从而捕捉输入序列的全局关系。

3. **解码器的自注意力（Decoder Self-Attention）：**
   - 解码器中的每个解码器层也包含自注意力层。这使得解码器中的每个位置都能够关注该位置及之前的所有位置，以保持自回归特性。为了防止左向信息流，对 softmax 输入中与非法连接对应的值进行屏蔽。

总体而言，多头注意力在 Transformer 中的应用主要包括解码器与编码器的交互（编码器-解码器注意力）以及模型内部的自注意力（编码器的自注意力和解码器的自注意力）。这些注意力机制使得模型能够在处理序列数据时更好地捕捉全局依赖关系。


### 位置编码前馈网络
Position-wise Feed-Forward Networks（位置编码前馈网络）用于在自注意力机制的层之后对每个位置的表示进行非线性变换。

在 Transformer 中，每个编码器和解码器层包含两个主要子层：自注意力层（Self-Attention Layer）和位置编码前馈网络。自注意力层用于处理输入序列的全局关系，而位置编码前馈网络则用于在每个位置独立地处理局部特征。

位置编码前馈网络的结构如下：

1. **输入：** 对应自注意力层输出的每个位置的表示。
2. **操作：** 应用两个全连接层，分别是一个经过 ReLU 激活函数的隐藏层和一个线性输出层。
3. **输出：** 得到每个位置的新表示。

这个前馈网络的目的是引入非线性变换，允许模型在每个位置上独立地处理特征。这对于捕捉一些局部模式和特定位置的信息非常重要。

在公式上，位置编码前馈网络可以表示为：

$\text{FFN}(x) = \text{ReLU}(\text{W}_2 \cdot \text{ReLU}(\text{W}_1 \cdot x + \text{b}_1) + \text{b}_2)$

其中，$\text{W}_1$、$\text{W}_2$ 分别是两个全连接层的权重矩阵，$\text{b}_1$、$\text{b}_2$ 是偏置向量，$\text{ReLU}$ 表示修正线性单元的激活函数。

位置编码前馈网络的引入使得 Transformer 能够更灵活地学习不同位置的特征表示，从而提高模型的表达能力。
### 
## 什么是 LLM？LLM 解决了什么问题？
LLM 是